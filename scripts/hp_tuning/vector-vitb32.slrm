#!/bin/bash

#SBATCH --job-name=pmc15m
#SBATCH --partition=a40
#SBATCH --qos=a40_arashaf_multimodal
#SBATCH --mem-per-gpu=32GB
#SBATCH --time=72:00:00
#SBATCH --nodes=1
#SBATCH --gres=gpu:4
#SBATCH --ntasks-per-node=4
#SBATCH --cpus-per-task=4
#SBATCH --export=ALL
#SBATCH --output=../outputs/slurm-%j-%N.out
#SBATCH --open-mode=append

# load virtual environment
source ~/Documents/envs/mmm/bin/activate

cd ../../src
export PYTHONPATH="."

export NCCL_IB_DISABLE=1  # disable InfiniBand (the Vector cluster does not have it)
export NCCL_DEBUG=WARN
export NCCL_DEBUG_SUBSYS=WARN
export NCCL_ASYNC_ERROR_HANDLING=1 # set to 1 for NCCL backend
export CUDA_LAUNCH_BLOCKING=1
export TORCH_DISTRIBUTED_DEBUG=DETAIL
export HYDRA_FULL_ERROR=1
export OMP_NUM_THREADS=1

export MASTER_ADDR=$(hostname)
export MASTER_PORT=45678

nvidia-smi
echo SLURM_ARRAY_JOB_ID=${SLURM_ARRAY_JOB_ID}
echo SLURM_JOBID=${SLURM_JOBID}

# “srun” executes the script <ntasks-per-node * nodes> times
srun --export=ALL -N $SLURM_JOB_NUM_NODES --cpu_bind=v --accel-bind=gn python -u training/main.py \
        --model ViT-B-32 \
        --pretrained openai \
        --train-data /projects/multimodal/datasets/pmc_oa/train.jsonl::/projects/multimodal/datasets/Quilt_1M/quilt_1m_train.csv::/projects/multimodal/datasets/mimic_cxr/mimic_cxr_double_image_train.csv::/projects/aieng/multimodal/datasets/roco/cache/radiologytraindata.csv \
        --train-num-samples 2769337 \
        --dataset-type mixed \
        --csv-separator , \
        --val-data /projects/multimodal/datasets/pmc_oa/valid.jsonl \
        --val-no-retrieval \
        --batch-size 128 \
        --accum-freq 4 \
        --workers 4 \
        --lr 1e-4 \
        --lr-scheduler cosine \
        --epochs 100 \
        --warmup 10 \
        --aug-cfg quilt_crop=True \
        --wd 0.1 \
        --name gpt77-ViT-B-32-ALL \
        --resume latest \
        --gather-with-grad \
        --logs /checkpoint/$USER/$SLURM_JOBID/ \
        --zeroshot-frequency 1 \
        --report-to wandb
